# Environment Configuration Guide

This guide explains the environment configuration setup for the agents-running-a-data-mesh-demo project.

## Overview

The project uses a **two-tier .env configuration structure**:

1. **Root `.env`** - For Terraform infrastructure provisioning (organization-level credentials)
2. **`agents/.env`** - For agent runtime (cluster-specific credentials from Terraform outputs)

## Configuration Files

### Root Directory

#### `.env.example`
Template file showing required variables for Terraform operations.

#### `.env` (gitignored)
User-created file containing actual credentials for infrastructure provisioning.

**Variables:**
- `CONFLUENT_CLOUD_API_KEY` - Cloud API Key from Confluent Cloud Console
- `CONFLUENT_CLOUD_API_SECRET` - Cloud API Secret
- `ANTHROPIC_API_KEY` - Claude API key for agent operations
- `CLAUDE_BACKEND` - Backend choice: `anthropic` or `bedrock`
- `AWS_REGION` - AWS region (if using Bedrock)
- `AWS_ACCESS_KEY_ID` - AWS credentials (if using Bedrock)
- `AWS_SECRET_ACCESS_KEY` - AWS credentials (if using Bedrock)

**Format:** Shell export statements (`export KEY=value`)

**Usage:**
```bash
source .env
cd terraform && terraform apply
```

### Agents Directory

#### `agents/.env.example`
Template file showing required variables for agent runtime.

#### `agents/.env` (gitignored)
User-created file containing cluster-specific credentials from Terraform outputs.

**Variables:**
- `KAFKA_BOOTSTRAP_ENDPOINT` - Kafka cluster bootstrap endpoint
- `KAFKA_API_KEY` - Cluster-specific API key for Kafka operations
- `KAFKA_API_SECRET` - Cluster-specific API secret
- `SCHEMA_REGISTRY_URL` - Schema Registry REST endpoint
- `SCHEMA_REGISTRY_API_KEY` - Schema Registry API key
- `SCHEMA_REGISTRY_API_SECRET` - Schema Registry API secret
- `CONFLUENT_CLOUD_API_KEY` - Org-level Cloud API key (for metrics/monitoring)
- `CONFLUENT_CLOUD_API_SECRET` - Org-level Cloud API secret
- `ANTHROPIC_API_KEY` - Claude API key
- `KAFKA_CLUSTER_ID` (optional) - Cluster ID for API operations
- `KAFKA_ENVIRONMENT_ID` (optional) - Environment ID for API operations

**Format:** Standard KEY=value pairs (no export)

**Usage:** All agents (discovery, monitoring, etc.) read from this file

## Setup Workflow

### Step 1: Create Root .env

```bash
# Copy the example file
cp .env.example .env

# Edit .env and add your credentials
# Get Cloud API Keys from: Confluent Cloud Console → Cloud API Keys
nano .env
```

### Step 2: Provision Infrastructure

```bash
# Load credentials and run Terraform
source .env
cd terraform
terraform init
terraform apply
```

### Step 3: Create Agents .env

After Terraform completes, extract the outputs and populate `agents/.env`:

```bash
# View Terraform outputs
terraform output

# Copy agent template
cd ..
cp agents/.env.example agents/.env

# Edit agents/.env with Terraform outputs
nano agents/.env
```

**Key Terraform Outputs to Copy:**
- `kafka_bootstrap_endpoint` → `KAFKA_BOOTSTRAP_ENDPOINT`
- `kafka_api_key` → `KAFKA_API_KEY`
- `kafka_api_secret` → `KAFKA_API_SECRET`
- `schema_registry_rest_endpoint` → `SCHEMA_REGISTRY_URL`
- `schema_registry_api_key` → `SCHEMA_REGISTRY_API_KEY`
- `schema_registry_api_secret` → `SCHEMA_REGISTRY_API_SECRET`

### Step 4: Run Agents

```bash
cd agents/discovery
python discovery_agent.py
```

Agents automatically load credentials from `agents/.env`.

## Credential Types

### Cloud API Keys (Organization-Level)
- **Source:** Confluent Cloud Console → Cloud API Keys
- **Permissions:** Organization-wide access for provisioning and management
- **Used by:** Terraform, agent monitoring/metrics collection
- **Scope:** All environments and clusters

### Cluster API Keys (Cluster-Specific)
- **Source:** Generated by Terraform or manually in cluster settings
- **Permissions:** Read/write access to specific Kafka cluster
- **Used by:** Agent data operations (produce, consume, topic admin)
- **Scope:** Single cluster only

## Security Best Practices

### 1. Never Commit .env Files
The `.gitignore` is configured to exclude all `.env` files except `.env.example` templates.

Verify with:
```bash
git status
# Should NOT show .env files
```

### 2. Rotate Compromised Credentials
If credentials are accidentally exposed:

**For Cloud API Keys:**
1. Go to Confluent Cloud Console → Cloud API Keys
2. Delete compromised key
3. Create new key
4. Update root `.env` and `agents/.env`
5. Re-run `terraform apply` if needed

**For Cluster API Keys:**
1. Delete compromised key in Confluent Cloud
2. Generate new key via Terraform or console
3. Update `agents/.env`

### 3. Use Separate Keys for Different Purposes
- Use different Cloud API Keys for CI/CD vs local development
- Use different cluster keys per agent if implementing least-privilege access

### 4. Validate .env Files
Before committing:
```bash
# Check for accidentally staged .env files
git status

# Verify .gitignore is working
git check-ignore .env agents/.env
# Should output: .env and agents/.env
```

## Troubleshooting

### Issue: "Authentication failed" errors
**Cause:** Missing or incorrect credentials in `.env` files

**Solution:**
1. Verify `.env` files exist and have correct values
2. For agents, ensure Terraform outputs were copied correctly
3. Check credential type (Cloud API vs Cluster API) matches usage

### Issue: Agents can't find .env file
**Cause:** Agent looking in wrong directory

**Solution:**
All agents should use `agents/.env`, not subdirectory-specific .env files. The discovery agent inherits from parent `agents/.env`.

### Issue: Terraform can't find credentials
**Cause:** Root `.env` not loaded into shell

**Solution:**
```bash
source .env
# Verify with:
echo $CONFLUENT_CLOUD_API_KEY
```

### Issue: .env file committed to git
**Cause:** File was staged before `.gitignore` was updated

**Solution:**
```bash
# Remove from git without deleting local file
git rm --cached .env
git rm --cached agents/.env

# Commit the removal
git commit -m "fix: remove .env files from git tracking"
```

## Directory Structure

```
/repo-root/
├── .env                          # Terraform credentials (gitignored)
├── .env.example                  # Terraform template (committed)
├── terraform/
│   └── (reads credentials from shell env via source .env)
└── agents/
    ├── .env                      # Agent runtime credentials (gitignored)
    ├── .env.example              # Agent template (committed)
    └── discovery/
        └── (inherits from parent agents/.env)
```

## FAQ

**Q: Why two separate .env files?**

A: Different tools have different requirements:
- Terraform needs shell exports for provider configuration
- Python agents use libraries that read KEY=value format
- Separating concerns prevents credential mixing and confusion

**Q: Can I use one .env file for everything?**

A: Not recommended. The root `.env` has org-level permissions that shouldn't be needed by agents. Keep cluster credentials separate for security.

**Q: Do I need to recreate agents/.env after every terraform apply?**

A: Only if you destroy and recreate the cluster. For updates to existing infrastructure, credentials remain the same.

**Q: Can agents share the same .env file?**

A: Yes! All agents read from `agents/.env`. Don't create subdirectory-specific .env files.

**Q: What if I'm using Bedrock instead of Anthropic API?**

A: Set `CLAUDE_BACKEND=bedrock` in root `.env` and provide AWS credentials. Copy `ANTHROPIC_API_KEY` to `agents/.env` anyway (some agents may need it).
