# Environment Configuration Guide

This guide explains the environment configuration setup for the agents-running-a-data-mesh-demo project.

## Overview

The project uses a **two-tier .env configuration structure**:

1. **Root `.env`** - For Terraform infrastructure provisioning (organization-level credentials only)
2. **`agents/.env`** - For bootstrap.py and agent runtime (cluster-specific credentials + Anthropic API key)

## Configuration Files

### Root Directory

#### `.env.example`
Template file showing required variables for Terraform operations.

#### `.env` (gitignored)
User-created file containing actual Confluent Cloud credentials for infrastructure provisioning.

**Variables (only 2):**
- `CONFLUENT_CLOUD_API_KEY` - Organization-level Cloud API Key from Confluent Cloud Console
- `CONFLUENT_CLOUD_API_SECRET` - Organization-level Cloud API Secret

**Format:** Shell export statements (`export KEY=value`)

**Usage:**
```bash
source .env
cd terraform && terraform apply
```

### Agents Directory

#### `agents/.env.example`
Template file showing required variables for agent runtime.

#### `agents/.env` (gitignored)
User-created file containing cluster-specific credentials from Terraform outputs plus Anthropic API key.

**Variables:**
- `KAFKA_BOOTSTRAP_ENDPOINT` - Kafka cluster bootstrap endpoint
- `KAFKA_API_KEY` - Cluster-specific API key for Kafka operations
- `KAFKA_API_SECRET` - Cluster-specific API secret
- `SCHEMA_REGISTRY_URL` - Schema Registry REST endpoint
- `SCHEMA_REGISTRY_API_KEY` - Schema Registry API key
- `SCHEMA_REGISTRY_API_SECRET` - Schema Registry API secret
- `ANTHROPIC_API_KEY` - Claude API key (get from https://console.anthropic.com/)
- `KAFKA_CLUSTER_ID` (optional) - Cluster ID for API operations
- `KAFKA_ENVIRONMENT_ID` (optional) - Environment ID for API operations

**Format:** Standard KEY=value pairs (no export)

**Usage:** bootstrap.py and all monitoring/ideation agents read from this file

## Setup Workflow

### Step 1: Create Root .env

```bash
# Copy the example file
cp .env.example .env

# Edit .env and add your credentials
# Get Cloud API Keys from: Confluent Cloud Console → Cloud API Keys
nano .env
```

### Step 2: Provision Infrastructure

```bash
# Load credentials and run Terraform
source .env
cd terraform
terraform init
terraform apply
```

### Step 3: Create Agents .env

After Terraform completes, use the setup script to automatically populate `agents/.env`:

```bash
# Option 1: Use the setup script (recommended)
./scripts/setup-agents-env.sh
# This will extract Terraform outputs and prompt for your Anthropic API key

# Option 2: Manual setup
terraform output      # View outputs
cd ..
cp agents/.env.example agents/.env
nano agents/.env      # Copy values manually
```

**The setup script automatically extracts:**
- `kafka_bootstrap_endpoint` → `KAFKA_BOOTSTRAP_ENDPOINT`
- `kafka_api_key` → `KAFKA_API_KEY`
- `kafka_api_secret` → `KAFKA_API_SECRET`
- `schema_registry_rest_endpoint` → `SCHEMA_REGISTRY_URL`
- `schema_registry_api_key` → `SCHEMA_REGISTRY_API_KEY`
- `schema_registry_api_secret` → `SCHEMA_REGISTRY_API_SECRET`
- `cluster_id` → `KAFKA_CLUSTER_ID`
- `environment_id` → `KAFKA_ENVIRONMENT_ID`
- Prompts you for `ANTHROPIC_API_KEY`

### Step 4: Run Bootstrap

```bash
cd agents
python bootstrap.py
```

bootstrap.py automatically loads credentials from `agents/.env` and runs all monitoring agents in sequence.

## Credential Types

### Cloud API Keys (Organization-Level)
- **Source:** Confluent Cloud Console → Cloud API Keys
- **Permissions:** Organization-wide access for provisioning and management
- **Used by:** Terraform only
- **Scope:** All environments and clusters

### Cluster API Keys (Cluster-Specific)
- **Source:** Generated by Terraform or manually in cluster settings
- **Permissions:** Read/write access to specific Kafka cluster
- **Used by:** Agent data operations (produce, consume, topic admin)
- **Scope:** Single cluster only

## Security Best Practices

### 1. Never Commit .env Files
The `.gitignore` is configured to exclude all `.env` files except `.env.example` templates.

Verify with:
```bash
git status
# Should NOT show .env files
```

### 2. Rotate Compromised Credentials
If credentials are accidentally exposed:

**For Cloud API Keys:**
1. Go to Confluent Cloud Console → Cloud API Keys
2. Delete compromised key
3. Create new key
4. Update root `.env` and `agents/.env`
5. Re-run `terraform apply` if needed

**For Cluster API Keys:**
1. Delete compromised key in Confluent Cloud
2. Generate new key via Terraform or console
3. Update `agents/.env`

### 3. Use Separate Keys for Different Purposes
- Use different Cloud API Keys for CI/CD vs local development
- Use different cluster keys per agent if implementing least-privilege access

### 4. Validate .env Files
Before committing:
```bash
# Check for accidentally staged .env files
git status

# Verify .gitignore is working
git check-ignore .env agents/.env
# Should output: .env and agents/.env
```

## Troubleshooting

### Issue: "Authentication failed" errors
**Cause:** Missing or incorrect credentials in `.env` files

**Solution:**
1. Verify `.env` files exist and have correct values
2. For agents, ensure Terraform outputs were copied correctly
3. Check credential type (Cloud API vs Cluster API) matches usage

### Issue: Agents can't find .env file
**Cause:** .env file missing or in wrong location

**Solution:**
bootstrap.py and all agents expect `agents/.env` in the agents directory. Make sure you've created it from `agents/.env.example` and populated it with Terraform outputs.

### Issue: Terraform can't find credentials
**Cause:** Root `.env` not loaded into shell

**Solution:**
```bash
source .env
# Verify with:
echo $CONFLUENT_CLOUD_API_KEY
```

### Issue: .env file committed to git
**Cause:** File was staged before `.gitignore` was updated

**Solution:**
```bash
# Remove from git without deleting local file
git rm --cached .env
git rm --cached agents/.env

# Commit the removal
git commit -m "fix: remove .env files from git tracking"
```

## Directory Structure

```
/repo-root/
├── .env                          # Terraform credentials (gitignored)
├── .env.example                  # Terraform template (committed)
├── terraform/
│   └── (reads credentials from shell env via source .env)
└── agents/
    ├── .env                      # Agent runtime credentials (gitignored)
    ├── .env.example              # Agent template (committed)
    ├── bootstrap.py              # Main entry point for agents
    ├── monitoring/               # Deployment, Usage, Metrics agents
    └── ideation/                 # Current State agent
```

## FAQ

**Q: Why two separate .env files?**

A: Different tools have different requirements:
- Terraform needs shell exports for provider configuration
- Python agents use libraries that read KEY=value format
- Separating concerns prevents credential mixing and confusion

**Q: Can I use one .env file for everything?**

A: Not recommended. The root `.env` only has Confluent Cloud credentials for Terraform. The agents need additional credentials (Kafka cluster, Schema Registry, Anthropic API) that Terraform doesn't use.

**Q: Do I need to recreate agents/.env after every terraform apply?**

A: Only if you destroy and recreate the cluster. For updates to existing infrastructure, credentials remain the same.

**Q: Where does bootstrap.py run from?**

A: Run it from the `agents/` directory: `cd agents && python bootstrap.py`. It will automatically load `agents/.env`.

**Q: What's the difference between Cloud API Keys and Cluster API Keys?**

A: Cloud API Keys are org-level (used by Terraform). Cluster API Keys are cluster-specific (used by agents to read/write Kafka data).
